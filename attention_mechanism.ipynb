{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: datasets in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (3.3.2)\n",
      "Requirement already satisfied: filelock in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
      "Requirement already satisfied: aiohttp in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from datasets) (3.11.7)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from datasets) (0.29.1)\n",
      "Requirement already satisfied: packaging in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: subword-nmt in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (0.3.8)\n",
      "Requirement already satisfied: mock in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from subword-nmt) (5.1.0)\n",
      "Requirement already satisfied: tqdm in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from subword-nmt) (4.67.1)\n",
      "Requirement already satisfied: tokenizers in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (0.21.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from tokenizers) (0.29.1)\n",
      "Requirement already satisfied: filelock in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n",
    "!pip install datasets\n",
    "!pip install subword-nmt\n",
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the pre-processed dataset\n",
    "dataset = load_dataset(\"stas/wmt14-en-de-pre-processed\",verification_mode=\"no_checks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4548885"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train']['translation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']['translation']\n",
    "val_dataset = dataset['validation']['translation']\n",
    "test_dataset = dataset['test']['translation']\n",
    "\n",
    "def create_file(dataset, file_name):\n",
    "    with open(file_name + \".txt\",\"w\",encoding=\"utf-8\") as f:\n",
    "        for item in dataset:\n",
    "            f.write('[DE] ' + item['de'] + '\\n')\n",
    "            f.write('[EN] ' + item['en'] + '\\n')\n",
    "\n",
    "create_file(train_dataset, \"train\")\n",
    "create_file(val_dataset, \"val\")\n",
    "create_file(test_dataset, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(vocab_size=37000, show_progress=True, special_tokens=[\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\",\"[DE]\",\"[EN]\"], min_frequency=2, continuing_subword_prefix=\"@@\")\n",
    "tokenizer.train(files=[\"train.txt\"], trainer=trainer)\n",
    "tokenizer.save(\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7072, 8397, 6671, 20789]\n",
      "['Ich', 'bin', 'ein', 'Berliner']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Ich bin ein Berliner\"\n",
    "print(tokenizer.encode(sentence).ids)\n",
    "print(tokenizer.encode(sentence).tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INPUT EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "#we need to move vocab size as a hardcode input\n",
    "\n",
    "class InputEmbedding(nn.Module):\n",
    "    \"\"\"_summary_:Performs normalized embedddings. Returns N x d_k dimensions\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_k):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_k = d_k\n",
    "        #This will create hypervectors for the whole vocab_size with dimension d_k\n",
    "        self.embedding = nn.Embedding(vocab_size, d_k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #the reason why we multiply by sqrt(d_k) is because we compute dot products of the embeddings\n",
    "        #they would grow up very large so we need to normalize by d_k\n",
    "        return self.embedding(x)*torch.sqrt(torch.tensor(self.d_k))\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x is the input embedding\n",
    "        pos = torch.arange(x.size(1)).unsqueeze(1)\n",
    "        i = torch.arange(x.size(2))        \n",
    "        print(pos.shape,i.shape)\n",
    "        elements = pos/torch.pow(10000, 2*i/x.size(2))\n",
    "        print(elements.shape)\n",
    "        self.pe = torch.zeros(x.shape)\n",
    "        self.pe[:,:,1::2] = torch.sin(elements[:,0::2])\n",
    "        self.pe[:,:,0::2] = torch.cos(elements[:,1::2])\n",
    "        \n",
    "        return x+self.pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs:  tensor([[ 7072,  8397,  6671, 20789]])\n",
      "Embeddings:  torch.Size([1, 4, 10])\n",
      "torch.Size([4, 1]) torch.Size([10])\n",
      "torch.Size([4, 10])\n",
      "Encodings torch.Size([1, 4, 10])\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Ich bin ein Berliner\"\n",
    "ids = tokenizer.encode(sentence).ids\n",
    "ids = torch.tensor(ids).unsqueeze(0)\n",
    "print(\"IDs: \", ids)\n",
    "input_embedding = InputEmbedding(37000, 10)\n",
    "embeddings = input_embedding(ids)\n",
    "print(\"Embeddings: \", embeddings.shape)\n",
    "positional_encoding = PositionalEncoding()\n",
    "position_encoded = positional_encoding(embeddings)\n",
    "print(\"Encodings\",position_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch\n",
    "\n",
    "class ScaledDotProductAttention():\n",
    "    \"\"\"_summary_:Performs scaled dot product attention\n",
    "       _inputs_: input_embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, d_k):\n",
    "        self.d_k = d_k\n",
    "        self.scale = 1/torch.sqrt(torch.tensor(d_k))\n",
    "        self.linear_Q = nn.Linear(d_k, d_k)\n",
    "        self.linear_K = nn.Linear(d_k, d_k)\n",
    "        self.linear_V = nn.Linear(d_k, d_k)\n",
    "        \n",
    "    def forward(self, Q, K, V):\n",
    "        #we need to project the query, key and value matrices to the same dimension\n",
    "        Q = self.linear_Q(Q)\n",
    "        K = self.linear_K(K)\n",
    "        V = self.linear_V(V)\n",
    "        #Q and k are the query, key and value matrices (Batch, Nsamples, d_k)\n",
    "        #we multiply each embedding dimension by the other embedding dimensions (dot product)\n",
    "        #it learns the importance of each embedding dimension for the other embedding dimensions\n",
    "        K_transpose = K.transpose(-2, -1) * self.scale\n",
    "        attention_scores = torch.matmul(Q, K_transpose)\n",
    "        #along the last dimension, we apply softmax\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        updated_values = torch.matmul(attention_weights, V)\n",
    "        return updated_values, attention_weights\n",
    "\n",
    "class MultiHeadAttention():\n",
    "    \"\"\"_summary_:Performs multihead attention\n",
    "       _inputs_: input_embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "\n",
    "class Encoder():\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
