{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install quietly\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "!pip install --quiet sentencepiece\n",
    "!pip install --quiet datasets\n",
    "!pip install --quiet subword-nmt\n",
    "!pip install --quiet tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ia4118/anaconda3/envs/pytorch_venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the pre-processed dataset\n",
    "dataset = load_dataset(\"stas/wmt14-en-de-pre-processed\",verification_mode=\"no_checks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4548885"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train']['translation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']['translation']\n",
    "val_dataset = dataset['validation']['translation']\n",
    "test_dataset = dataset['test']['translation']\n",
    "\n",
    "def create_file(dataset, file_name):\n",
    "    with open(file_name + \".txt\",\"w\",encoding=\"utf-8\") as f:\n",
    "        for item in dataset:\n",
    "            f.write('[DE] ' + item['de'] + '\\n')\n",
    "            f.write('[EN] ' + item['en'] + '\\n')\n",
    "\n",
    "create_file(train_dataset, \"train\")\n",
    "create_file(val_dataset, \"val\")\n",
    "create_file(test_dataset, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(vocab_size=37000, show_progress=True, special_tokens=[\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\",\"[DE]\",\"[EN]\"], min_frequency=2, continuing_subword_prefix=\"@@\")\n",
    "tokenizer.train(files=[\"train.txt\"], trainer=trainer)\n",
    "tokenizer.save(\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7072, 8397, 6671, 20789]\n",
      "['Ich', 'bin', 'ein', 'Berliner']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Ich bin ein Berliner\"\n",
    "print(tokenizer.encode(sentence).ids)\n",
    "print(tokenizer.encode(sentence).tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INPUT EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "#we need to move vocab size as a hardcode input\n",
    "\n",
    "class InputEmbedding(nn.Module):\n",
    "    \"\"\"_summary_:Performs normalized embedddings. Returns N x d_k dimensions\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_k):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_k = d_k\n",
    "        #This will create hypervectors for the whole vocab_size with dimension d_k\n",
    "        self.embedding = nn.Embedding(vocab_size, d_k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #the reason why we multiply by sqrt(d_k) is because we compute dot products of the embeddings\n",
    "        #they would grow up very large so we need to normalize by d_k\n",
    "        return self.embedding(x)*torch.sqrt(torch.tensor(self.d_k))\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x is the input embedding\n",
    "        pos = torch.arange(x.size(1)).unsqueeze(1)\n",
    "        i = torch.arange(x.size(2))        \n",
    "        print(pos.shape,i.shape)\n",
    "        elements = pos/torch.pow(10000, 2*i/x.size(2))\n",
    "        print(elements.shape)\n",
    "        self.pe = torch.zeros(x.shape)\n",
    "        self.pe[:,:,1::2] = torch.sin(elements[:,0::2])\n",
    "        self.pe[:,:,0::2] = torch.cos(elements[:,1::2])\n",
    "        \n",
    "        return x+self.pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs:  tensor([[ 7072,  8397,  6671, 20789]])\n",
      "Embeddings:  torch.Size([1, 4, 10])\n",
      "torch.Size([4, 1]) torch.Size([10])\n",
      "torch.Size([4, 10])\n",
      "Encodings torch.Size([1, 4, 10])\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Ich bin ein Berliner\"\n",
    "ids = tokenizer.encode(sentence).ids\n",
    "ids = torch.tensor(ids).unsqueeze(0)\n",
    "print(\"IDs: \", ids)\n",
    "input_embedding = InputEmbedding(37000, 10)\n",
    "embeddings = input_embedding(ids)\n",
    "print(\"Embeddings: \", embeddings.shape)\n",
    "positional_encoding = PositionalEncoding()\n",
    "position_encoded = positional_encoding(embeddings)\n",
    "print(\"Encodings\",position_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"_summary_:Performs multihead attention\n",
    "       _inputs_: input_embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.linear_values = nn.Linear(d_model, d_model)\n",
    "        self.Q_proj = nn.Linear(d_model, d_model)\n",
    "        self.K_proj = nn.Linear(d_model, d_model)\n",
    "        self.V_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self,query,key,value):\n",
    "        #we need to project the query, key and value matrices to the same dimension\n",
    "        batch_size, seq_len, d_model = query.shape\n",
    "        Q = self.Q_proj(query)\n",
    "        K = self.K_proj(key)\n",
    "        V = self.V_proj(value)\n",
    "        \n",
    "        self.d_k = d_model//self.num_heads\n",
    "        self.d_v = d_model//self.num_heads\n",
    "        self.scale = 1/torch.sqrt(torch.tensor(self.d_k))\n",
    "        \n",
    "        #We add head dimension so we split the embeddings into different heads\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_v).transpose(1, 2)\n",
    "        \n",
    "        #dimensions are now batch_size, num_heads, seq_len, d_k/d_v\n",
    "        attention_weights = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        attention_weights = torch.softmax(attention_weights, dim=-1)\n",
    "        \n",
    "        #attention weights dimensions are now batch_size, num_heads, seq_len, seq_len\n",
    "        values = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        #values is now batch_size, num_heads, seq_len, d_v\n",
    "        values = values.transpose(1,2)\n",
    "        \n",
    "        #values is now batch_size, seq_len, num_heads, d_v\n",
    "        #contiguous() is used to make the tensor contiguous in memory otherwise view() will fail\n",
    "        values = values.contiguous().view(batch_size, seq_len, d_model)\n",
    "        values = self.linear_values(values)\n",
    "        return values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"_summary_:Performs one encoder layer\n",
    "       _inputs_: d_model, num_heads\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x + self.multi_head_attention(query=x,key=x,value=x))\n",
    "        x = self.norm2(x + self.feed_forward(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"_summary_:Performs one encoder layer\n",
    "       _inputs_: d_model, num_heads, num_layers, input_embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "        self.encoder_blocks = nn.ModuleList([EncoderBlock(d_model, num_heads) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            x = encoder_block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"_summary_:Performs one decoder layer\n",
    "       _inputs_: d_model, num_heads\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.multi_head_attention_2 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, d_model)\n",
    "        )\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, encoder_output):\n",
    "        x = self.norm1(x + self.multi_head_attention(query=x,key=x,value=x))\n",
    "        x = self.norm2(x + self.multi_head_attention_2(query=encoder_output,key=encoder_output,value=x))\n",
    "        x = self.norm3(x + self.feed_forward(x))\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"_summary_:Performs one decoder layer\n",
    "       _inputs_: d_model, num_heads, num_layers\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "        self.decoder_blocks = nn.ModuleList([DecoderBlock(d_model, num_heads) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x, encoder_output):\n",
    "        for decoder_block in self.decoder_blocks:\n",
    "            x = decoder_block(x, encoder_output)\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"_summary_:Performs one transformer layer\n",
    "       _inputs_: d_model, num_heads, num_layers, input_embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, num_layers,vocab_size):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(d_model, num_heads, num_layers)\n",
    "        self.decoder = Decoder(d_model, num_heads, num_layers)\n",
    "        self.linear_projection = nn.Linear(d_model, d_model)\n",
    "        self.input_embedding = InputEmbedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding()\n",
    "    \n",
    "    def forward(self, input, output):\n",
    "        x1 = self.input_embedding(input)\n",
    "        x1 = self.positional_encoding(x1)\n",
    "        x1 = self.encoder(x1)\n",
    "        \n",
    "        x2 = self.input_embedding(output)\n",
    "        x2 = self.positional_encoding(x2)\n",
    "        x2 = self.decoder(x2, x1)\n",
    "        \n",
    "        x2 = self.linear_projection(x2)\n",
    "        return x2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
